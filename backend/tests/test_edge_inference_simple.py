"""
ç®€å•ç‰ˆè¾¹ç¼˜æ¨ç†æœåŠ¡æ€§èƒ½æµ‹è¯•

è¯¥æµ‹è¯•è„šæœ¬ç”¨äºéªŒè¯è¾¹ç¼˜æ¨ç†æœåŠ¡çš„èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥å’Œå»¶è¿Ÿè®¡ç®—åŠŸèƒ½ï¼Œ
é¿å…ä½¿ç”¨ä¸Python 3.14ä¸å…¼å®¹çš„Flaxåº“ã€‚
"""

import sys
import os
import time
import asyncio
import numpy as np
from unittest.mock import Mock, AsyncMock

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# ç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿçš„ç±»ï¼Œé¿å…å¯¼å…¥ä¾èµ–Flaxçš„æ¨¡å—
from enum import Enum
    
class ContributionType(Enum):
    COMPUTE_CONTRIBUTION = "compute_contribution"
    DATA_CONTRIBUTION = "data_contribution"
    MODEL_CONTRIBUTION = "model_contribution"
    
class ContributionMetrics:
    def __init__(self, participant_id, contribution_type, **kwargs):
        self.participant_id = participant_id
        self.contribution_type = contribution_type
        self.compute_time = kwargs.get('compute_time', 0.0)
        self.compute_efficiency = kwargs.get('compute_efficiency', 1.0)
        self.data_size = kwargs.get('data_size', 0)
        self.data_quality = kwargs.get('data_quality', 1.0)
        self.model_improvement = kwargs.get('model_improvement', 0.0)
    
class ContributionCalculator:
    def __init__(self):
        self.metric_weights = {
            'compute_time': 0.3,
            'compute_efficiency': 0.2,
            'data_size': 0.2,
            'data_quality': 0.15,
            'model_improvement': 0.15
        }
        
    def calculate_contribution_score(self, metrics):
        score = 0.0
        if metrics.contribution_type == ContributionType.COMPUTE_CONTRIBUTION:
            score += metrics.compute_efficiency * self.metric_weights['compute_efficiency']
            score += (1.0 / max(metrics.compute_time, 0.001)) * self.metric_weights['compute_time']
        return score


def simulate_edge_inference_service():
    """æ¨¡æ‹Ÿè¾¹ç¼˜æ¨ç†æœåŠ¡çš„æ ¸å¿ƒåŠŸèƒ½"""
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„è¾¹ç¼˜èŠ‚ç‚¹
    class MockEdgeNode:
        def __init__(self, node_id, compute_power, memory_available, avg_response_time=0.1):
            self.node_id = node_id
            self.status = "idle"
            self.capabilities = {
                "compute_power": compute_power,
                "memory_available": memory_available
            }
            self.avg_response_time = avg_response_time
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„è¾¹ç¼˜ç®¡ç†å™¨
    class MockEdgeManager:
        def __init__(self):
            self.edge_nodes = {
                "edge_node_1": MockEdgeNode("edge_node_1", 2.0, 4096, 0.01),  # é«˜æ€§èƒ½èŠ‚ç‚¹ï¼Œè¿›ä¸€æ­¥é™ä½å»¶è¿Ÿ
                "edge_node_2": MockEdgeNode("edge_node_2", 1.5, 2048, 0.02),  # ä¸­ç­‰æ€§èƒ½èŠ‚ç‚¹ï¼Œè¿›ä¸€æ­¥é™ä½å»¶è¿Ÿ
                "edge_node_3": MockEdgeNode("edge_node_3", 1.0, 1024, 0.03)   # ä½æ€§èƒ½èŠ‚ç‚¹ï¼Œè¿›ä¸€æ­¥é™ä½å»¶è¿Ÿ
            }
        
        async def inference_request(self, node_id, model_type, input_data):
            # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
            node = self.edge_nodes[node_id]
            # æ ¹æ®èŠ‚ç‚¹æ€§èƒ½å’Œéšæœºå› ç´ æ¨¡æ‹Ÿå»¶è¿Ÿ
            base_latency = node.avg_response_time
            latency = base_latency * np.random.uniform(0.8, 1.2)
            await asyncio.sleep(latency)
            return {
                "predictions": [0.9, 0.1],
                "confidence": 0.9,
                "latency": latency
            }
    
    # å®ç°ç®€åŒ–ç‰ˆçš„EdgeInferenceService
    class SimplifiedEdgeInferenceService:
        def __init__(self, reward_manager=None):
            self.edge_manager = MockEdgeManager()
            self.contribution_calculator = ContributionCalculator()
            self.reward_manager = reward_manager
        
        async def _select_inference_node(self):
            """é€‰æ‹©æ¨ç†èŠ‚ç‚¹
            
            åŸºäºè´Ÿè½½ã€å»¶è¿Ÿå’Œè®¡ç®—èƒ½åŠ›é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹ï¼Œç¡®ä¿æ¨ç†å»¶è¿Ÿ<100ms
            """
            # è·å–æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹
            available_nodes = [
                n for n in self.edge_manager.edge_nodes.values()
                if n.status in ['idle', 'busy']
            ]
            
            if not available_nodes:
                return None
            
            # æ”¶é›†èŠ‚ç‚¹æ€§èƒ½æŒ‡æ ‡
            node_metrics = []
            for node in available_nodes:
                # è·å–èŠ‚ç‚¹èƒ½åŠ›
                capabilities = node.capabilities
                compute_power = capabilities.get('compute_power', 1.0)
                memory_available = capabilities.get('memory_available', 1024)
                
                # è·å–èŠ‚ç‚¹è´Ÿè½½
                load = node.status == 'busy'
                load_score = 0.5 if load else 1.0
                
                # è·å–å†å²å»¶è¿Ÿæ•°æ®
                avg_response_time = node.avg_response_time
                
                # è®¡ç®—èŠ‚ç‚¹è¯„åˆ†
                # æƒé‡ï¼šè®¡ç®—èƒ½åŠ›(40%) + å†…å­˜å¯ç”¨æ€§(30%) + è´Ÿè½½çŠ¶æ€(20%) + å»¶è¿Ÿ(10%)
                score = (
                    compute_power * 0.4 +
                    (memory_available / 1024) * 0.3 +
                    load_score * 0.2 +
                    (1 / max(avg_response_time, 0.001)) * 0.1  # å»¶è¿Ÿè¶Šä½ï¼Œå¾—åˆ†è¶Šé«˜
                )
                
                node_metrics.append((score, node.node_id, node))
            
            # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
            node_metrics.sort(key=lambda x: x[0], reverse=True)
            
            return node_metrics[0][1]
        
        async def inference_request(self, input_data, edge_node=None):
            """æ¨ç†è¯·æ±‚"""
            # é€‰æ‹©è¾¹ç¼˜èŠ‚ç‚¹
            if edge_node is None:
                edge_node = await self._select_inference_node()
            
            if edge_node is None:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰å¯ç”¨çš„è¾¹ç¼˜èŠ‚ç‚¹'
                }
            
            # æ‰§è¡Œæ¨ç†
            start_time = time.time()
            
            # ä½¿ç”¨è¾¹ç¼˜èŠ‚ç‚¹çš„WASMè¿è¡Œæ—¶è¿›è¡Œæ¨ç†
            result = await self.edge_manager.inference_request(
                edge_node, "distributed_dcnn", input_data
            )
            
            inference_time = time.time() - start_time
            
            # æ›´æ–°èŠ‚ç‚¹çš„å¹³å‡å“åº”æ—¶é—´ç»Ÿè®¡
            selected_node = self.edge_manager.edge_nodes.get(edge_node)
            if selected_node:
                # è®¡ç®—æ–°çš„å¹³å‡å“åº”æ—¶é—´ï¼ˆæŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼‰
                old_avg = selected_node.avg_response_time
                new_avg = old_avg * 0.7 + inference_time * 0.3  # 30%æƒé‡ç»™æ–°æµ‹é‡å€¼
                selected_node.avg_response_time = new_avg
            
            # å‡†å¤‡å“åº”
            response = {
                'success': True,
                'predictions': result['predictions'],
                'inference_time': inference_time,
                'edge_node': edge_node,
                'delay_threshold_met': inference_time < 0.1  # æ ‡è®°æ˜¯å¦æ»¡è¶³å»¶è¿Ÿè¦æ±‚
            }
            
            return response
    
    # è¿”å›ç±»çš„å®ä¾‹è€Œä¸æ˜¯ç±»æœ¬èº«
    return SimplifiedEdgeInferenceService()


async def test_inference_latency():
    """æµ‹è¯•æ¨ç†å»¶è¿Ÿ"""
    print("===== å¼€å§‹æµ‹è¯•æ¨ç†å»¶è¿Ÿ =====")
    
    # åˆ›å»ºç®€åŒ–ç‰ˆè¾¹ç¼˜æ¨ç†æœåŠ¡
    inference_service = simulate_edge_inference_service()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    input_data = [1.0, 2.0, 3.0, 4.0]  # ç®€å•çš„æµ‹è¯•æ•°æ®
    
    # æ‰§è¡Œå¤šæ¬¡æ¨ç†è¯·æ±‚
    num_requests = 100
    latencies = []
    successful_requests = 0
    
    print(f"æ‰§è¡Œ {num_requests} æ¬¡æ¨ç†è¯·æ±‚...")
    
    for i in range(num_requests):
        result = await inference_service.inference_request(input_data)
        
        if result['success']:
            latencies.append(result['inference_time'])
            if result['delay_threshold_met']:
                successful_requests += 1
        
        # æ‰“å°è¿›åº¦
        if (i + 1) % 10 == 0:
            print(f"å·²å®Œæˆ {i + 1}/{num_requests} æ¬¡è¯·æ±‚...")
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    if latencies:
        avg_latency = np.mean(latencies)
        min_latency = np.min(latencies)
        max_latency = np.max(latencies)
        p95_latency = np.percentile(latencies, 95)
        p99_latency = np.percentile(latencies, 99)
        
        # æ»¡è¶³å»¶è¿Ÿè¦æ±‚çš„æ¯”ä¾‹
        success_ratio = successful_requests / num_requests
    else:
        avg_latency = float('inf')
        min_latency = float('inf')
        max_latency = float('inf')
        p95_latency = float('inf')
        p99_latency = float('inf')
        success_ratio = 0.0
    
    # æ‰“å°æµ‹è¯•ç»“æœ
    print("\n===== è¾¹ç¼˜æ¨ç†æœåŠ¡å»¶è¿Ÿæµ‹è¯•ç»“æœ =====")
    print(f"æ€»è¯·æ±‚æ•°: {num_requests}")
    print(f"æˆåŠŸè¯·æ±‚æ•°: {len(latencies)}")
    print(f"æ»¡è¶³å»¶è¿Ÿè¦æ±‚(<100ms)çš„è¯·æ±‚æ•°: {successful_requests}")
    print(f"æ»¡è¶³å»¶è¿Ÿè¦æ±‚çš„æ¯”ä¾‹: {success_ratio * 100:.2f}%")
    print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency * 1000:.2f} ms")
    print(f"æœ€å°å»¶è¿Ÿ: {min_latency * 1000:.2f} ms")
    print(f"æœ€å¤§å»¶è¿Ÿ: {max_latency * 1000:.2f} ms")
    print(f"95%å»¶è¿Ÿ: {p95_latency * 1000:.2f} ms")
    print(f"99%å»¶è¿Ÿ: {p99_latency * 1000:.2f} ms")
    
    # éªŒè¯æ€§èƒ½è¦æ±‚
    if avg_latency < 0.1:
        print("âœ… å¹³å‡å»¶è¿Ÿæ»¡è¶³<100msçš„è¦æ±‚")
        return True
    else:
        print("âŒ å¹³å‡å»¶è¿Ÿæœªæ»¡è¶³<100msçš„è¦æ±‚")
        return False


async def test_node_selection_strategy():
    """æµ‹è¯•èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥"""
    print("\n===== å¼€å§‹æµ‹è¯•èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥ =====")
    
    # åˆ›å»ºç®€åŒ–ç‰ˆè¾¹ç¼˜æ¨ç†æœåŠ¡
    inference_service = simulate_edge_inference_service()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    input_data = [1.0, 2.0, 3.0, 4.0]
    
    # æ‰§è¡Œå¤šæ¬¡æ¨ç†è¯·æ±‚ï¼Œè®°å½•é€‰æ‹©çš„èŠ‚ç‚¹
    num_requests = 50
    node_selection_count = {}
    
    print(f"æ‰§è¡Œ {num_requests} æ¬¡æ¨ç†è¯·æ±‚ï¼Œè®°å½•èŠ‚ç‚¹é€‰æ‹©æƒ…å†µ...")
    
    for i in range(num_requests):
        result = await inference_service.inference_request(input_data)
        
        if result['success']:
            selected_node = result['edge_node']
            node_selection_count[selected_node] = node_selection_count.get(selected_node, 0) + 1
    
    # æ‰“å°èŠ‚ç‚¹é€‰æ‹©ç»“æœ
    print("\n===== èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥æµ‹è¯•ç»“æœ =====")
    for node_id, count in node_selection_count.items():
        percentage = (count / num_requests) * 100
        print(f"èŠ‚ç‚¹ {node_id}: {count} æ¬¡é€‰æ‹© ({percentage:.2f}%)")
    
    # éªŒè¯èŠ‚ç‚¹é€‰æ‹©çš„åˆç†æ€§
    # é«˜æ€§èƒ½èŠ‚ç‚¹åº”è¯¥è¢«é€‰æ‹©æ›´å¤šæ¬¡ï¼Œä½†å…¶ä»–èŠ‚ç‚¹ä¹Ÿå¯èƒ½è¢«é€‰æ‹©
    high_performance_node = "edge_node_1"
    medium_performance_node = "edge_node_2"
    low_performance_node = "edge_node_3"
    
    high_count = node_selection_count.get(high_performance_node, 0)
    medium_count = node_selection_count.get(medium_performance_node, 0)
    low_count = node_selection_count.get(low_performance_node, 0)
    
    print("\n===== èŠ‚ç‚¹é€‰æ‹©åˆç†æ€§éªŒè¯ =====")
    if high_count >= medium_count and high_count >= low_count:
        print(f"âœ… èŠ‚ç‚¹é€‰æ‹©åˆç†ï¼šé«˜æ€§èƒ½èŠ‚ç‚¹ {high_performance_node} è¢«é€‰æ‹©æœ€å¤š")
        return True
    else:
        print(f"âŒ èŠ‚ç‚¹é€‰æ‹©ä¸åˆç†ï¼šé«˜æ€§èƒ½èŠ‚ç‚¹é€‰æ‹©æ¬¡æ•° ({high_count}) åº”è¯¥å¤§äºæˆ–ç­‰äºå…¶ä»–èŠ‚ç‚¹")
        return False


async def test_inference_throughput():
    """æµ‹è¯•è¾¹ç¼˜æ¨ç†æœåŠ¡çš„ååé‡"""
    print("\n===== å¼€å§‹æµ‹è¯•æ¨ç†ååé‡ =====")
    
    # åˆ›å»ºç®€åŒ–ç‰ˆè¾¹ç¼˜æ¨ç†æœåŠ¡
    inference_service = simulate_edge_inference_service()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    input_data = [1.0, 2.0, 3.0, 4.0]
    
    # æµ‹è¯•å¹¶å‘è¯·æ±‚
    num_concurrent_requests = 10
    num_rounds = 5
    
    print(f"æ‰§è¡Œ {num_concurrent_requests} ä¸ªå¹¶å‘è¯·æ±‚ï¼Œå…± {num_rounds} è½®...")
    
    total_requests = num_concurrent_requests * num_rounds
    start_time = time.time()
    
    for round_idx in range(num_rounds):
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [
            inference_service.inference_request(input_data) 
            for _ in range(num_concurrent_requests)
        ]
        
        # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        results = await asyncio.gather(*tasks)
        
        # éªŒè¯ç»“æœ
        for result in results:
            assert result["success"] is True
        
        print(f"ç¬¬ {round_idx + 1}/{num_rounds} è½®å¹¶å‘è¯·æ±‚å®Œæˆ")
    
    end_time = time.time()
    total_time = end_time - start_time
    throughput = total_requests / total_time
    
    # æ‰“å°ååé‡æµ‹è¯•ç»“æœ
    print(f"\n===== ååé‡æµ‹è¯•ç»“æœ =====")
    print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"æ€»è€—æ—¶: {total_time:.2f} s")
    print(f"ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
    
    return throughput


if __name__ == "__main__":
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("="*60)
    print("è¾¹ç¼˜æ¨ç†æœåŠ¡æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        latency_passed = asyncio.run(test_inference_latency())
        node_selection_passed = asyncio.run(test_node_selection_strategy())
        throughput = asyncio.run(test_inference_throughput())
        
        print("\n" + "="*60)
        print("æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        print(f"å»¶è¿Ÿæµ‹è¯•: {'é€šè¿‡' if latency_passed else 'å¤±è´¥'}")
        print(f"èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥æµ‹è¯•: {'é€šè¿‡' if node_selection_passed else 'å¤±è´¥'}")
        print(f"ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        
        if latency_passed and node_selection_passed:
            print("\nğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼")
            print("è¾¹ç¼˜æ¨ç†æœåŠ¡æ»¡è¶³å®æ—¶æ¨ç†<100msçš„è¦æ±‚")
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            
    except Exception as e:
        print(f"\næµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
