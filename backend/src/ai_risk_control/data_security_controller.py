"""
æ•°æ®å®‰å…¨ä¸éšç§ä¿æŠ¤é£é™©æ§åˆ¶æ¨¡å—

è´Ÿè´£ä¿æŠ¤AIè®­ç»ƒå’Œå†³ç­–è¿‡ç¨‹ä¸­çš„æ•°æ®å®‰å…¨ï¼Œé˜²æ­¢éšç§æ³„éœ²ï¼Œ
ç¡®ä¿é“¾ä¸Šé“¾ä¸‹æ•°æ®çš„æœºå¯†æ€§å’Œå®Œæ•´æ€§ï¼Œé˜²èŒƒæ•°æ®çªƒå–å’Œæ»¥ç”¨é£é™©ã€‚
"""

import logging
import asyncio
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import hashlib
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC


class DataRiskType(Enum):
    """æ•°æ®é£é™©ç±»å‹"""
    PRIVACY_LEAKAGE = "privacy_leakage"  # éšç§æ³„éœ²
    DATA_BREACH = "data_breach"  # æ•°æ®æ³„éœ²
    UNAUTHORIZED_ACCESS = "unauthorized_access"  # æœªæˆæƒè®¿é—®
    MODEL_PARAM_LEAKAGE = "model_param_leakage"  # æ¨¡å‹å‚æ•°æ³„éœ²
    FEDERATED_LEARNING_ATTACK = "federated_learning_attack"  # è”é‚¦å­¦ä¹ æ”»å‡»


class DataRiskSeverity(Enum):
    """æ•°æ®é£é™©ä¸¥é‡ç¨‹åº¦"""
    LOW = "low"
    MEDIUM = "medium" 
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class DataSecurityAlert:
    """æ•°æ®å®‰å…¨è­¦æŠ¥"""
    risk_type: DataRiskType
    severity: DataRiskSeverity
    alert_id: str
    description: str
    affected_data: Dict[str, Any]
    confidence_score: float
    protection_action: str
    timestamp: datetime


@dataclass
class DataSecurityAssessment:
    """æ•°æ®å®‰å…¨è¯„ä¼°ç»“æœ"""
    overall_security_level: DataRiskSeverity
    security_score: float  # 0-1ä¹‹é—´çš„å®‰å…¨è¯„åˆ†
    active_alerts: List[DataSecurityAlert]
    encryption_status: bool
    privacy_protection_status: bool
    compliance_status: bool
    recommendations: List[str]


class DataSecurityController:
    """æ•°æ®å®‰å…¨ä¸éšç§ä¿æŠ¤æ§åˆ¶å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or self._get_default_config()
        
        # å®‰å…¨ç›‘æ§çŠ¶æ€
        self.security_metrics = {}
        self.alert_history = []
        self.encryption_keys = {}
        self.access_logs = []
        
        # éšç§ä¿æŠ¤ç»„ä»¶
        self.privacy_protector = PrivacyProtector()
        
        # åŠ å¯†ç®¡ç†å™¨
        self.encryption_manager = EncryptionManager()
        
        # è®¿é—®æ§åˆ¶ç®¡ç†å™¨
        self.access_controller = AccessController()
        
        # è”é‚¦å­¦ä¹ å®‰å…¨å™¨
        self.federated_security = FederatedLearningSecurity()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "privacy_leakage_threshold": 0.7,  # éšç§æ³„éœ²é£é™©é˜ˆå€¼
            "data_breach_threshold": 0.8,  # æ•°æ®æ³„éœ²é£é™©é˜ˆå€¼
            "unauthorized_access_threshold": 3,  # æœªæˆæƒè®¿é—®æ¬¡æ•°é˜ˆå€¼
            "model_param_protection_threshold": 0.6,  # æ¨¡å‹å‚æ•°ä¿æŠ¤é˜ˆå€¼
            "federated_learning_security_threshold": 0.5,  # è”é‚¦å­¦ä¹ å®‰å…¨é˜ˆå€¼
            "encryption_required": True,  # æ˜¯å¦è¦æ±‚åŠ å¯†
            "privacy_preservation_enabled": True,  # æ˜¯å¦å¯ç”¨éšç§ä¿æŠ¤
            "access_control_enabled": True,  # æ˜¯å¦å¯ç”¨è®¿é—®æ§åˆ¶
            "audit_log_retention_days": 30  # å®¡è®¡æ—¥å¿—ä¿ç•™å¤©æ•°
        }
    
    async def assess_data_security_risk(self,
                                      training_data: Dict[str, Any],
                                      model_parameters: Dict[str, Any],
                                      blockchain_context: Dict[str, Any],
                                      access_logs: List[Dict[str, Any]]) -> DataSecurityAssessment:
        """
        è¯„ä¼°æ•°æ®å®‰å…¨ä¸éšç§ä¿æŠ¤é£é™©
        
        Args:
            training_data: è®­ç»ƒæ•°æ®ä¿¡æ¯
            model_parameters: æ¨¡å‹å‚æ•°ä¿¡æ¯
            blockchain_context: åŒºå—é“¾ä¸Šä¸‹æ–‡
            access_logs: è®¿é—®æ—¥å¿—
            
        Returns:
            DataSecurityAssessment: æ•°æ®å®‰å…¨è¯„ä¼°ç»“æœ
        """
        try:
            alerts = []
            
            # 1. éšç§æ³„éœ²é£é™©è¯„ä¼°
            privacy_leakage_risk = await self._assess_privacy_leakage_risk(
                training_data, blockchain_context)
            alerts.extend(privacy_leakage_risk)
            
            # 2. æ•°æ®æ³„éœ²é£é™©è¯„ä¼°
            data_breach_risk = await self._assess_data_breach_risk(
                training_data, model_parameters, access_logs)
            alerts.extend(data_breach_risk)
            
            # 3. æœªæˆæƒè®¿é—®é£é™©è¯„ä¼°
            unauthorized_access_risk = await self._assess_unauthorized_access_risk(
                access_logs)
            alerts.extend(unauthorized_access_risk)
            
            # 4. æ¨¡å‹å‚æ•°æ³„éœ²é£é™©è¯„ä¼°
            model_param_leakage_risk = await self._assess_model_param_leakage_risk(
                model_parameters, blockchain_context)
            alerts.extend(model_param_leakage_risk)
            
            # 5. è”é‚¦å­¦ä¹ æ”»å‡»é£é™©è¯„ä¼°
            federated_learning_risk = await self._assess_federated_learning_risk(
                training_data, model_parameters)
            alerts.extend(federated_learning_risk)
            
            # 6. ç»¼åˆå®‰å…¨è¯„ä¼°
            overall_security_level = self._determine_overall_security_level(alerts)
            security_score = self._calculate_security_score(alerts)
            
            # 7. æ£€æŸ¥åŠ å¯†çŠ¶æ€
            encryption_status = await self._check_encryption_status(training_data, model_parameters)
            
            # 8. æ£€æŸ¥éšç§ä¿æŠ¤çŠ¶æ€
            privacy_protection_status = await self._check_privacy_protection_status(training_data)
            
            # 9. ç”Ÿæˆæ”¹è¿›å»ºè®®
            recommendations = self._generate_recommendations(alerts, overall_security_level)
            
            # 10. æ›´æ–°å®‰å…¨æŒ‡æ ‡
            self._update_security_metrics(alerts, overall_security_level)
            
            return DataSecurityAssessment(
                overall_security_level=overall_security_level,
                security_score=security_score,
                active_alerts=alerts,
                encryption_status=encryption_status,
                privacy_protection_status=privacy_protection_status,
                compliance_status=overall_security_level in [DataRiskSeverity.LOW, DataRiskSeverity.MEDIUM],
                recommendations=recommendations
            )
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å®‰å…¨è¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›æœ€ä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°ç»“æœ
            return DataSecurityAssessment(
                overall_security_level=DataRiskSeverity.CRITICAL,
                security_score=0.0,
                active_alerts=[DataSecurityAlert(
                    risk_type=DataRiskType.DATA_BREACH,
                    severity=DataRiskSeverity.CRITICAL,
                    alert_id="ASSESSMENT_ERROR",
                    description="æ•°æ®å®‰å…¨è¯„ä¼°è¿‡ç¨‹å‡ºç°å¼‚å¸¸",
                    affected_data={"error": str(e)},
                    confidence_score=1.0,
                    protection_action="ç«‹å³åœæ­¢æ•°æ®å¤„ç†å¹¶æ£€æŸ¥ç³»ç»Ÿ",
                    timestamp=datetime.utcnow()
                )],
                encryption_status=False,
                privacy_protection_status=False,
                compliance_status=False,
                recommendations=["æ•°æ®å®‰å…¨è¯„ä¼°å¤±è´¥ï¼Œå»ºè®®ç«‹å³äººå·¥å¹²é¢„"]
            )
    
    async def _assess_privacy_leakage_risk(self,
                                         training_data: Dict[str, Any],
                                         blockchain_context: Dict[str, Any]) -> List[DataSecurityAlert]:
        """è¯„ä¼°éšç§æ³„éœ²é£é™©"""
        alerts = []
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®ä¸­çš„éšç§ä¿¡æ¯ä¿æŠ¤
        privacy_leakage_score = await self.privacy_protector.assess_privacy_risk(
            training_data, blockchain_context)
        
        if privacy_leakage_score > self.config["privacy_leakage_threshold"]:
            alerts.append(DataSecurityAlert(
                risk_type=DataRiskType.PRIVACY_LEAKAGE,
                severity=DataRiskSeverity.HIGH,
                alert_id="PRIVACY_LEAKAGE_001",
                description="æ£€æµ‹åˆ°éšç§ä¿¡æ¯æ³„éœ²é£é™©",
                affected_data={
                    "privacy_leakage_score": privacy_leakage_score,
                    "sensitive_fields": training_data.get("sensitive_fields", [])
                },
                confidence_score=privacy_leakage_score,
                protection_action="åº”ç”¨å·®åˆ†éšç§æˆ–æ•°æ®è„±æ•æŠ€æœ¯",
                timestamp=datetime.utcnow()
            ))
        
        return alerts
    
    async def _assess_data_breach_risk(self,
                                     training_data: Dict[str, Any],
                                     model_parameters: Dict[str, Any],
                                     access_logs: List[Dict[str, Any]]) -> List[DataSecurityAlert]:
        """è¯„ä¼°æ•°æ®æ³„éœ²é£é™©"""
        alerts = []
        
        # æ£€æŸ¥æ•°æ®å®‰å…¨é˜²æŠ¤æªæ–½
        data_breach_risk = await self._calculate_data_breach_risk(
            training_data, model_parameters, access_logs)
        
        if data_breach_risk > self.config["data_breach_threshold"]:
            alerts.append(DataSecurityAlert(
                risk_type=DataRiskType.DATA_BREACH,
                severity=DataRiskSeverity.CRITICAL,
                alert_id="DATA_BREACH_001",
                description="æ£€æµ‹åˆ°æ•°æ®æ³„éœ²é«˜é£é™©",
                affected_data={
                    "data_breach_risk": data_breach_risk,
                    "data_size": training_data.get("size", 0),
                    "encryption_status": training_data.get("encrypted", False)
                },
                confidence_score=data_breach_risk,
                protection_action="åŠ å¼ºæ•°æ®åŠ å¯†å’Œè®¿é—®æ§åˆ¶",
                timestamp=datetime.utcnow()
            ))
        
        return alerts
    
    async def _assess_unauthorized_access_risk(self, access_logs: List[Dict[str, Any]]) -> List[DataSecurityAlert]:
        """è¯„ä¼°æœªæˆæƒè®¿é—®é£é™©"""
        alerts = []
        
        # åˆ†æè®¿é—®æ—¥å¿—ï¼Œæ£€æµ‹å¼‚å¸¸è®¿é—®æ¨¡å¼
        unauthorized_attempts = await self.access_controller.detect_unauthorized_access(access_logs)
        
        if unauthorized_attempts >= self.config["unauthorized_access_threshold"]:
            alerts.append(DataSecurityAlert(
                risk_type=DataRiskType.UNAUTHORIZED_ACCESS,
                severity=DataRiskSeverity.HIGH,
                alert_id="UNAUTHORIZED_ACCESS_001",
                description="æ£€æµ‹åˆ°æœªæˆæƒè®¿é—®å°è¯•",
                affected_data={
                    "unauthorized_attempts": unauthorized_attempts,
                    "access_patterns": self._analyze_access_patterns(access_logs)
                },
                confidence_score=min(unauthorized_attempts / 10.0, 1.0),
                protection_action="å¼ºåŒ–èº«ä»½è®¤è¯å’Œè®¿é—®æ§åˆ¶",
                timestamp=datetime.utcnow()
            ))
        
        return alerts
    
    async def _assess_model_param_leakage_risk(self,
                                             model_parameters: Dict[str, Any],
                                             blockchain_context: Dict[str, Any]) -> List[DataSecurityAlert]:
        """è¯„ä¼°æ¨¡å‹å‚æ•°æ³„éœ²é£é™©"""
        alerts = []
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°ä¿æŠ¤æªæ–½
        model_protection_score = await self._assess_model_protection(model_parameters, blockchain_context)
        
        if model_protection_score < self.config["model_param_protection_threshold"]:
            alerts.append(DataSecurityAlert(
                risk_type=DataRiskType.MODEL_PARAM_LEAKAGE,
                severity=DataRiskSeverity.HIGH,
                alert_id="MODEL_PARAM_LEAKAGE_001",
                description="æ¨¡å‹å‚æ•°ä¿æŠ¤ä¸è¶³ï¼Œå­˜åœ¨æ³„éœ²é£é™©",
                affected_data={
                    "model_protection_score": model_protection_score,
                    "model_size": model_parameters.get("size", 0),
                    "protection_methods": model_parameters.get("protection_methods", [])
                },
                confidence_score=1.0 - model_protection_score,
                protection_action="åº”ç”¨æ¨¡å‹åŠ å¯†æˆ–è”é‚¦å­¦ä¹ æŠ€æœ¯",
                timestamp=datetime.utcnow()
            ))
        
        return alerts
    
    async def _assess_federated_learning_risk(self,
                                            training_data: Dict[str, Any],
                                            model_parameters: Dict[str, Any]) -> List[DataSecurityAlert]:
        """è¯„ä¼°è”é‚¦å­¦ä¹ æ”»å‡»é£é™©"""
        alerts = []
        
        # æ£€æŸ¥è”é‚¦å­¦ä¹ å®‰å…¨æ€§
        federated_security_score = await self.federated_security.assess_security_risk(
            training_data, model_parameters)
        
        if federated_security_score < self.config["federated_learning_security_threshold"]:
            alerts.append(DataSecurityAlert(
                risk_type=DataRiskType.FEDERATED_LEARNING_ATTACK,
                severity=DataRiskSeverity.MEDIUM,
                alert_id="FEDERATED_LEARNING_ATTACK_001",
                description="è”é‚¦å­¦ä¹ å®‰å…¨æ€§ä¸è¶³ï¼Œå­˜åœ¨æ”»å‡»é£é™©",
                affected_data={
                    "federated_security_score": federated_security_score,
                    "participant_count": training_data.get("participants", 0),
                    "aggregation_method": model_parameters.get("aggregation_method", "fedavg")
                },
                confidence_score=1.0 - federated_security_score,
                protection_action="åŠ å¼ºè”é‚¦å­¦ä¹ å®‰å…¨æœºåˆ¶",
                timestamp=datetime.utcnow()
            ))
        
        return alerts
    
    def _determine_overall_security_level(self, alerts: List[DataSecurityAlert]) -> DataRiskSeverity:
        """ç¡®å®šæ€»ä½“å®‰å…¨ç­‰çº§"""
        if not alerts:
            return DataRiskSeverity.LOW
        
        severities = [alert.severity for alert in alerts]
        
        if DataRiskSeverity.CRITICAL in severities:
            return DataRiskSeverity.CRITICAL
        elif DataRiskSeverity.HIGH in severities:
            return DataRiskSeverity.HIGH
        elif DataRiskSeverity.MEDIUM in severities:
            return DataRiskSeverity.MEDIUM
        else:
            return DataRiskSeverity.LOW
    
    def _calculate_security_score(self, alerts: List[DataSecurityAlert]) -> float:
        """è®¡ç®—ç»¼åˆå®‰å…¨è¯„åˆ†"""
        if not alerts:
            return 1.0
        
        severity_weights = {
            DataRiskSeverity.CRITICAL: 0.0,
            DataRiskSeverity.HIGH: 0.3,
            DataRiskSeverity.MEDIUM: 0.6,
            DataRiskSeverity.LOW: 0.9
        }
        
        total_score = sum(
            severity_weights[alert.severity] * (1.0 - alert.confidence_score)
            for alert in alerts
        )
        
        return total_score / len(alerts) if alerts else 1.0
    
    async def _check_encryption_status(self, training_data: Dict[str, Any], model_parameters: Dict[str, Any]) -> bool:
        """æ£€æŸ¥åŠ å¯†çŠ¶æ€"""
        if not self.config["encryption_required"]:
            return True
        
        # æ£€æŸ¥æ•°æ®å’Œæ¨¡å‹å‚æ•°æ˜¯å¦åŠ å¯†
        data_encrypted = training_data.get("encrypted", False)
        model_encrypted = model_parameters.get("encrypted", False)
        
        return data_encrypted and model_encrypted
    
    async def _check_privacy_protection_status(self, training_data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥éšç§ä¿æŠ¤çŠ¶æ€"""
        if not self.config["privacy_preservation_enabled"]:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åº”ç”¨äº†éšç§ä¿æŠ¤æŠ€æœ¯
        privacy_techniques = training_data.get("privacy_techniques", [])
        required_techniques = ["differential_privacy", "federated_learning", "homomorphic_encryption"]
        
        return any(tech in privacy_techniques for tech in required_techniques)
    
    def _generate_recommendations(self, 
                                alerts: List[DataSecurityAlert],
                                overall_security_level: DataRiskSeverity) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if overall_security_level == DataRiskSeverity.LOW:
            recommendations.append("æ•°æ®å®‰å…¨ä¿æŠ¤è‰¯å¥½ï¼Œç»§ç»­ä¿æŒç›‘æ§")
            return recommendations
        
        # æ ¹æ®é£é™©ç±»å‹æä¾›é’ˆå¯¹æ€§å»ºè®®
        risk_types = set(alert.risk_type for alert in alerts)
        
        if DataRiskType.PRIVACY_LEAKAGE in risk_types:
            recommendations.append("å»ºè®®åº”ç”¨å·®åˆ†éšç§æˆ–æ•°æ®è„±æ•æŠ€æœ¯")
        
        if DataRiskType.DATA_BREACH in risk_types:
            recommendations.append("å»ºè®®åŠ å¼ºæ•°æ®åŠ å¯†å’Œè®¿é—®æ§åˆ¶")
        
        if DataRiskType.UNAUTHORIZED_ACCESS in risk_types:
            recommendations.append("å»ºè®®å¼ºåŒ–èº«ä»½è®¤è¯å’Œæƒé™ç®¡ç†")
        
        if DataRiskType.MODEL_PARAM_LEAKAGE in risk_types:
            recommendations.append("å»ºè®®åº”ç”¨æ¨¡å‹åŠ å¯†æˆ–è”é‚¦å­¦ä¹ ")
        
        if DataRiskType.FEDERATED_LEARNING_ATTACK in risk_types:
            recommendations.append("å»ºè®®åŠ å¼ºè”é‚¦å­¦ä¹ å®‰å…¨æœºåˆ¶")
        
        # ç´§æ€¥æƒ…å†µå»ºè®®
        if overall_security_level in [DataRiskSeverity.HIGH, DataRiskSeverity.CRITICAL]:
            recommendations.insert(0, "ğŸ”’ é«˜å®‰å…¨é£é™©ï¼šå»ºè®®ç«‹å³å¯åŠ¨æ•°æ®ä¿æŠ¤åº”æ€¥æœºåˆ¶")
        
        return recommendations
    
    def _update_security_metrics(self, alerts: List[DataSecurityAlert], security_level: DataRiskSeverity):
        """æ›´æ–°å®‰å…¨æŒ‡æ ‡"""
        current_time = datetime.utcnow()
        
        # è®°å½•è­¦æŠ¥å†å²
        self.alert_history.extend(alerts)
        
        # æ¸…ç†è¿‡æ—¶è­¦æŠ¥ï¼ˆä¿ç•™é…ç½®çš„å¤©æ•°ï¼‰
        cutoff_time = current_time - timedelta(days=self.config["audit_log_retention_days"])
        self.alert_history = [
            alert for alert in self.alert_history 
            if alert.timestamp > cutoff_time
        ]
        
        # æ›´æ–°å®‰å…¨æŒ‡æ ‡
        self.security_metrics["last_assessment"] = current_time
        self.security_metrics["current_security_level"] = security_level
        self.security_metrics["active_alerts_count"] = len(alerts)
    
    # è¾…åŠ©æ–¹æ³•ï¼ˆç®€åŒ–å®ç°ï¼‰
    async def _calculate_data_breach_risk(self,
                                        training_data: Dict[str, Any],
                                        model_parameters: Dict[str, Any],
                                        access_logs: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•°æ®æ³„éœ²é£é™©"""
        risk_factors = []
        
        # æ•°æ®åŠ å¯†çŠ¶æ€
        data_encrypted = training_data.get("encrypted", False)
        risk_factors.append(0.0 if data_encrypted else 0.8)
        
        # è®¿é—®æ§åˆ¶å¼ºåº¦
        access_control_strength = training_data.get("access_control_strength", 0.5)
        risk_factors.append(1.0 - access_control_strength)
        
        # å¼‚å¸¸è®¿é—®æ¨¡å¼
        abnormal_access = await self._detect_abnormal_access(access_logs)
        risk_factors.append(abnormal_access)
        
        return sum(risk_factors) / len(risk_factors)
    
    async def _assess_model_protection(self,
                                     model_parameters: Dict[str, Any],
                                     blockchain_context: Dict[str, Any]) -> float:
        """è¯„ä¼°æ¨¡å‹ä¿æŠ¤ç¨‹åº¦"""
        protection_score = 0.0
        
        # æ¨¡å‹åŠ å¯†çŠ¶æ€
        if model_parameters.get("encrypted", False):
            protection_score += 0.3
        
        # å‚æ•°å®‰å…¨å­˜å‚¨
        if model_parameters.get("secure_storage", False):
            protection_score += 0.3
        
        # åŒºå—é“¾æº¯æºä¿æŠ¤
        if blockchain_context.get("model_tracking_enabled", False):
            protection_score += 0.4
        
        return protection_score
    
    async def _detect_abnormal_access(self, access_logs: List[Dict[str, Any]]) -> float:
        """æ£€æµ‹å¼‚å¸¸è®¿é—®æ¨¡å¼"""
        if not access_logs:
            return 0.0
        
        # åˆ†æè®¿é—®é¢‘ç‡ã€æ—¶é—´æ¨¡å¼ç­‰
        recent_logs = [log for log in access_logs 
                      if datetime.fromisoformat(log["timestamp"]) > datetime.utcnow() - timedelta(hours=24)]
        
        if len(recent_logs) > 100:  # å‡è®¾æ­£å¸¸è®¿é—®é¢‘ç‡é˜ˆå€¼
            return 0.7
        
        return 0.0
    
    def _analyze_access_patterns(self, access_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè®¿é—®æ¨¡å¼"""
        if not access_logs:
            return {}
        
        recent_logs = [log for log in access_logs 
                      if datetime.fromisoformat(log["timestamp"]) > datetime.utcnow() - timedelta(hours=24)]
        
        return {
            "total_accesses": len(recent_logs),
            "unique_users": len(set(log["user_id"] for log in recent_logs if "user_id" in log)),
            "access_frequency": len(recent_logs) / 24.0  # æ¯å°æ—¶è®¿é—®æ¬¡æ•°
        }


class PrivacyProtector:
    """éšç§ä¿æŠ¤å™¨"""
    
    async def assess_privacy_risk(self,
                                training_data: Dict[str, Any],
                                blockchain_context: Dict[str, Any]) -> float:
        """è¯„ä¼°éšç§é£é™©"""
        risk_score = 0.0
        
        # æ£€æŸ¥æ•æ„Ÿæ•°æ®å­—æ®µ
        sensitive_fields = training_data.get("sensitive_fields", [])
        if sensitive_fields:
            risk_score += 0.4
        
        # æ£€æŸ¥éšç§ä¿æŠ¤æŠ€æœ¯åº”ç”¨
        privacy_techniques = training_data.get("privacy_techniques", [])
        if not privacy_techniques:
            risk_score += 0.6
        
        return risk_score


class EncryptionManager:
    """åŠ å¯†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.fernet = None
        self._initialize_encryption()
    
    def _initialize_encryption(self):
        """åˆå§‹åŒ–åŠ å¯†ç³»ç»Ÿ"""
        # ç®€åŒ–å®ç°ï¼šå®é™…åº”ç”¨ä¸­åº”ä»å®‰å…¨å­˜å‚¨è·å–å¯†é’¥
        key = Fernet.generate_key()
        self.fernet = Fernet(key)
    
    async def encrypt_data(self, data: bytes) -> bytes:
        """åŠ å¯†æ•°æ®"""
        if self.fernet:
            return self.fernet.encrypt(data)
        return data
    
    async def decrypt_data(self, encrypted_data: bytes) -> bytes:
        """è§£å¯†æ•°æ®"""
        if self.fernet:
            return self.fernet.decrypt(encrypted_data)
        return encrypted_data


class AccessController:
    """è®¿é—®æ§åˆ¶å™¨"""
    
    async def detect_unauthorized_access(self, access_logs: List[Dict[str, Any]]) -> int:
        """æ£€æµ‹æœªæˆæƒè®¿é—®å°è¯•"""
        unauthorized_count = 0
        
        for log in access_logs:
            if log.get("access_granted", False) == False:
                unauthorized_count += 1
        
        return unauthorized_count


class FederatedLearningSecurity:
    """è”é‚¦å­¦ä¹ å®‰å…¨å™¨"""
    
    async def assess_security_risk(self,
                                 training_data: Dict[str, Any],
                                 model_parameters: Dict[str, Any]) -> float:
        """è¯„ä¼°è”é‚¦å­¦ä¹ å®‰å…¨é£é™©"""
        security_score = 0.0
        
        # æ£€æŸ¥å®‰å…¨èšåˆæœºåˆ¶
        if model_parameters.get("secure_aggregation", False):
            security_score += 0.4
        
        # æ£€æŸ¥å·®åˆ†éšç§åº”ç”¨
        if training_data.get("differential_privacy_applied", False):
            security_score += 0.3
        
        # æ£€æŸ¥å‚ä¸æ–¹è®¤è¯
        if training_data.get("participant_authentication", False):
            security_score += 0.3
        
        return security_score