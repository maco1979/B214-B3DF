# 前端性能深度优化计划

## 一、优化背景
当前系统是 **AI模型（注意力融合/域自适应/跨域迁移）+ 前端可视化分析（注意力权重热力图/分布分析/特征可视化）** 场景，核心痛点是：后端是「算力敏感型+大体积数据输出型」服务，前端是「大数据渲染+高频交互+可视化密集型」页面，前端性能问题不仅导致页面卡顿，更会反向加重后端压力、引发无效算力消耗、拖慢整体业务链路。

## 二、核心优化原则
1. **减负后端为第一优先级**：减少后端的无效请求、无效计算、无效数据传输
2. **前后端协同大于前端单兵作战**：前后端约定规范+双向适配
3. **算力合理迁移**：将非核心轻量计算从后端下沉到前端

## 三、优化内容

### 1. 请求层极致优化
- **精准按需请求**：确保只请求指定样本/层/头的注意力权重，避免全量数据拉取
- **智能缓存策略**：根据数据类型自动选择缓存类型和TTL，实现缓存版本控制
- **请求合并与防抖节流**：优化请求合并逻辑，支持更多场景；调整防抖节流参数以适应不同交互场景

### 2. 大体积数据传输专项优化
- **数据压缩与解压缩**：实现前端数据压缩解压缩功能，配合后端gzip/Brotli压缩
- **数据裁剪与精度控制**：只返回必要精度的数据（如保留3位小数），减少数据体积
- **二进制协议传输**：考虑使用Protocol Buffers或MessagePack替代JSON，提升传输效率

### 3. 可视化渲染层极致优化
- **热力图性能优化**：进一步优化AttentionHeatmap组件，支持更大seq_len的热力图渲染
- **智能虚拟渲染**：实现更智能的虚拟渲染，只渲染可见区域的数据
- **可视化库配置优化**：关闭不必要的交互和动画，提升渲染性能

### 4. 前后端协同异步解耦
- **后端负载感知**：实现前端感知后端负载，动态调整请求频率
- **异步任务管理优化**：完善useAsyncTask钩子，支持任务进度查询和取消
- **智能轮询策略**：根据任务类型和优先级调整轮询频率

### 5. 前端侧计算下沉
- **Web Worker能力扩展**：扩展attention-calculator.worker的计算能力，支持更多类型的计算
- **高效前后端计算分工**：将更多轻量计算下沉到前端，优化Web Worker与主线程之间的数据传输

### 6. 前端接口容错和资源保护
- **请求参数校验**：实现更完善的请求参数校验，拦截无效请求
- **指数退避重试**：实现指数退避重试策略，避免高频重试打满后端接口
- **请求熔断和降级**：实现请求熔断和降级机制，保护后端资源

## 四、实现方式

### 1. 请求层优化
- 优化现有API调用，确保只传递必要的参数
- 完善cache.ts，实现更智能的缓存策略
- 优化requestMerger.ts，支持更多场景的请求合并

### 2. 大体积数据传输优化
- 实现dataProcessor.ts的数据压缩和解压缩功能
- 与后端约定数据精度，只返回必要精度的数据
- 考虑使用二进制协议传输，需要前后端共同支持

### 3. 可视化渲染优化
- 优化AttentionHeatmap组件，进一步提升渲染性能
- 实现更智能的虚拟渲染逻辑
- 优化ECharts配置，关闭不必要的交互和动画

### 4. 前后端协同异步解耦
- 实现后端负载状态接口调用
- 完善useAsyncTask钩子，支持任务进度查询和取消
- 实现智能轮询策略

### 5. 前端侧计算下沉
- 扩展attention-calculator.worker，支持更多类型的计算
- 优化Web Worker与主线程之间的数据传输
- 实现更高效的前后端计算分工

### 6. 前端接口容错和资源保护
- 完善httpClient.ts，实现请求参数校验
- 实现指数退避重试策略
- 实现请求熔断和降级机制

## 五、预期效果

### 后端侧
- 接口QPS提升50%~100%
- 单接口平均响应时间减少60%~80%
- 后端CPU/内存使用率降低30%~50%
- 无内存溢出/进程阻塞情况

### 前端侧
- 注意力热力图加载速度从秒级→毫秒级
- 切换样本/层/头的响应时间≤100ms
- 页面内存占用降低40%+
- 长耗时任务不阻塞页面，可正常交互

### 整体链路
- 跨域模型推理+可视化分析的端到端耗时减少70%+
- 系统能承载的并发用户数提升2~3倍
- 模型的调试/优化效率大幅提升