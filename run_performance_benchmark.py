#!/usr/bin/env python3
"""
è¿ç§»å­¦ä¹ å’Œè¾¹ç¼˜è®¡ç®—é›†æˆæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
è‡ªåŠ¨åŒ–è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
"""

import sys
import json
import asyncio
import time
import argparse
from datetime import datetime
from pathlib import Path
from typing import Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ€§èƒ½æµ‹è¯•æ¨¡å—
try:
    from backend.src.performance.benchmark_test import BenchmarkTestSuite
    from backend.src.performance.performance_monitor import IntegrationPerformanceMonitor
    from backend.config.migration_edge_integration_config import (
        DeploymentEnvironment, OptimizationStrategy
    )
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ä¾èµ–å·²å®‰è£…å¹¶æ­£ç¡®é…ç½®")
    sys.exit(1)


class PerformanceBenchmarkRunner:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, environment: str = "production", strategy: str = "performance") -> None:
        self.environment: DeploymentEnvironment = DeploymentEnvironment(environment)
        self.strategy: OptimizationStrategy = OptimizationStrategy(strategy)
        self.benchmark_suite: BenchmarkTestSuite = BenchmarkTestSuite()
        self.performance_monitor: IntegrationPerformanceMonitor = IntegrationPerformanceMonitor()
        self.results: list[dict[str, Any]] = []
    
    async def run_migration_learning_benchmark(self) -> dict[str, Any]:
        """è¿è¡Œè¿ç§»å­¦ä¹ åŸºå‡†æµ‹è¯•"""
        print("ğŸ§  è¿è¡Œè¿ç§»å­¦ä¹ åŸºå‡†æµ‹è¯•...")
        
        # å®šä¹‰æµ‹è¯•åœºæ™¯
        test_scenarios: list[dict[str, Any]] = [
            {
                "name": "å†œä¸šå›¾åƒåˆ†ç±»è¿ç§»",
                "source_domain": "general_images",
                "target_domain": "agriculture_images",
                "data_size": 5000,
                "complexity": "medium",
                "data_quality": 0.9
            },
            {
                "name": "ä½œç‰©è¯†åˆ«è¿ç§»",
                "source_domain": "plant_images",
                "target_domain": "crop_images",
                "data_size": 3000,
                "complexity": "high",
                "data_quality": 0.85
            },
            {
                "name": "ç—…è™«å®³æ£€æµ‹è¿ç§»",
                "source_domain": "disease_images",
                "target_domain": "pest_images",
                "data_size": 2000,
                "complexity": "high",
                "data_quality": 0.8
            }
        ]
        
        result = await self.benchmark_suite.run_migration_learning_benchmark(test_scenarios)
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        accuracy: float = float(result.metrics.get("average_accuracy", 0.85))
        await self.performance_monitor.record_migration_learning_performance(
            source_domain="benchmark",
            target_domain="benchmark",
            accuracy=accuracy,
            baseline_accuracy=0.8,
            processing_time=result.duration
        )
        
        return {
            "test_type": "migration_learning",
            "result": result,
            "scenarios": test_scenarios
        }
    
    async def run_edge_computing_benchmark(self) -> dict[str, Any]:
        """è¿è¡Œè¾¹ç¼˜è®¡ç®—åŸºå‡†æµ‹è¯•"""
        print("âš¡ è¿è¡Œè¾¹ç¼˜è®¡ç®—åŸºå‡†æµ‹è¯•...")
        
        # å®šä¹‰è¾¹ç¼˜èŠ‚ç‚¹
        edge_nodes: list[dict[str, Any]] = [
            {
                "node_id": "edge_node_01",
                "cpu_cores": 4,
                "memory_gb": 8,
                "storage_gb": 64,
                "network_bandwidth": 100
            },
            {
                "node_id": "edge_node_02", 
                "cpu_cores": 2,
                "memory_gb": 4,
                "storage_gb": 32,
                "network_bandwidth": 50
            }
        ]
        
        # å®šä¹‰è®¡ç®—ä»»åŠ¡
        tasks: list[dict[str, Any]] = [
            {
                "task_id": "real_time_inference",
                "computational_intensity": 2,
                "data_size": 10,
                "latency_requirement": 100
            },
            {
                "task_id": "batch_processing",
                "computational_intensity": 5,
                "data_size": 100,
                "latency_requirement": 500
            },
            {
                "task_id": "model_training",
                "computational_intensity": 8,
                "data_size": 1000,
                "latency_requirement": 1000
            }
        ]
        
        result = await self.benchmark_suite.run_edge_computing_benchmark(edge_nodes, tasks)
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        edge_latency: float = float(result.metrics.get("average_edge_latency", 0))
        cloud_latency: float = float(result.metrics.get("average_cloud_latency", 0))
        await self.performance_monitor.record_edge_computing_performance(
            node_id="benchmark_node",
            task_type="benchmark",
            edge_latency=edge_latency,
            cloud_latency=cloud_latency,
            resource_utilization={
                "cpu": 0.7,
                "memory": 0.6,
                "storage": 0.3
            }
        )
        
        return {
            "test_type": "edge_computing",
            "result": result,
            "nodes": edge_nodes,
            "tasks": tasks
        }
    
    async def run_integration_benchmark(self) -> dict[str, Any]:
        """è¿è¡Œé›†æˆåŸºå‡†æµ‹è¯•"""
        print("ğŸ”— è¿è¡Œé›†æˆåŸºå‡†æµ‹è¯•...")
        
        # å®šä¹‰é›†æˆåœºæ™¯
        integration_scenarios: list[dict[str, Any]] = [
            {
                "name": "è¿ç§»å­¦ä¹ +è¾¹ç¼˜æ¨ç†",
                "integration_complexity": "high",
                "components": ["migration_learning", "edge_computing"],
                "data_flow": "cloud_to_edge"
            },
            {
                "name": "å®æ—¶å†³ç­–é›†æˆ",
                "integration_complexity": "medium", 
                "components": ["decision_engine", "performance_monitor"],
                "data_flow": "edge_to_cloud"
            },
            {
                "name": "ç«¯åˆ°ç«¯å·¥ä½œæµ",
                "integration_complexity": "high",
                "components": ["migration_learning", "edge_computing", "decision_engine"],
                "data_flow": "hybrid"
            }
        ]
        
        result = await self.benchmark_suite.run_integration_benchmark(integration_scenarios)
        
        # è®°å½•é›†æˆæ€§èƒ½æŒ‡æ ‡
        await self.performance_monitor.record_integration_metric(
            integration_type="benchmark",
            operation="integration_test",
            duration=result.duration,
            success=True,
            additional_tags={
                "scenarios_count": len(integration_scenarios),
                "success_rate": result.success_rate
            }
        )
        
        return {
            "test_type": "integration",
            "result": result,
            "scenarios": integration_scenarios
        }
    
    async def run_comprehensive_benchmark(self) -> dict[str, Any]:
        """è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•"""
        print("ğŸ¯ è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•...")
        
        start_time = time.time()
        
        # å¹¶è¡Œè¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
        migration_result, edge_result, integration_result = await asyncio.gather(
            self.run_migration_learning_benchmark(),
            self.run_edge_computing_benchmark(), 
            self.run_integration_benchmark(),
            return_exceptions=True
        )
        
        end_time = time.time()
        
        # å¤„ç†ç»“æœ
        results: list[dict[str, Any]] = []
        for result in [migration_result, edge_result, integration_result]:
            if isinstance(result, Exception):
                print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {result}")
                results.append({"error": str(result)})
            else:
                results.append(result)
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = self._calculate_overall_score(results)
        
        return {
            "timestamp": datetime.now().isoformat(),
            "environment": self.environment.value,
            "strategy": self.strategy.value,
            "duration": end_time - start_time,
            "overall_score": overall_score,
            "results": results,
            "performance_level": self._get_performance_level(overall_score)
        }
    
    def _calculate_overall_score(self, results: list[dict[str, Any]]) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        if not results:
            return 0.0
        
        total_score: float = 0.0
        valid_results = 0
        
        for result in results:
            if "error" not in result and "result" in result:
                benchmark_result = result["result"]
                
                # åŸºäºæˆåŠŸç‡å’Œååé‡è®¡ç®—åˆ†æ•°
                success_rate: float = float(benchmark_result.success_rate)
                throughput: float = float(benchmark_result.throughput)
                
                success_score: float = success_rate * 0.6
                throughput_score: float = min(throughput / 10, 1.0) * 0.4
                
                total_score += (success_score + throughput_score)
                valid_results += 1
        
        return total_score / valid_results if valid_results > 0 else 0.0
    
    def _get_performance_level(self, score: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 0.9:
            return "ä¼˜ç§€"
        elif score >= 0.7:
            return "è‰¯å¥½"
        elif score >= 0.5:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    def generate_detailed_report(self, benchmark_results: dict[str, Any]) -> dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        
        report: dict[str, Any] = {
            "benchmark_info": {
                "timestamp": benchmark_results["timestamp"],
                "environment": benchmark_results["environment"],
                "strategy": benchmark_results["strategy"],
                "total_duration": benchmark_results["duration"],
                "overall_score": benchmark_results["overall_score"],
                "performance_level": benchmark_results["performance_level"]
            },
            "test_results": {},
            "performance_analysis": {},
            "recommendations": []
        }
        
        # åˆ†æå„ä¸ªæµ‹è¯•ç»“æœ
        for result in benchmark_results["results"]:
            if "error" in result:
                test_type: str = str(result.get("test_type", "unknown"))
                report["test_results"][test_type] = {
                    "status": "failed",
                    "error": result["error"]
                }
            else:
                test_type = str(result["test_type"])
                test_result = result["result"]
                report["test_results"][test_type] = {
                    "status": "completed",
                    "duration": float(test_result.duration),
                    "success_rate": float(test_result.success_rate),
                    "throughput": float(test_result.throughput),
                    "metrics": test_result.metrics
                }
        
        # æ€§èƒ½åˆ†æ
        report["performance_analysis"] = self._analyze_performance(report["test_results"])
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        report["recommendations"] = self._generate_recommendations(report)
        
        return report
    
    def _analyze_performance(self, test_results: dict[str, Any]) -> dict[str, list[str]]:
        """åˆ†ææ€§èƒ½æ•°æ®"""
        
        analysis: dict[str, list[str]] = {
            "strengths": [],
            "weaknesses": [],
            "bottlenecks": [],
            "opportunities": []
        }
        
        for test_type, result in test_results.items():
            if result["status"] != "completed":
                continue
            
            # åˆ†æä¼˜åŠ¿
            success_rate: float = float(result["success_rate"])
            throughput: float = float(result["throughput"])
            
            if success_rate > 0.9:
                analysis["strengths"].append(f"{test_type} æˆåŠŸç‡ä¼˜ç§€ ({success_rate:.1%})")
            
            if throughput > 5.0:
                analysis["strengths"].append(f"{test_type} ååé‡è‰¯å¥½ ({throughput:.1f} æ“ä½œ/ç§’)")
            
            # åˆ†æå¼±ç‚¹
            if success_rate < 0.7:
                analysis["weaknesses"].append(f"{test_type} æˆåŠŸç‡è¾ƒä½ ({success_rate:.1%})")
            
            if throughput < 1.0:
                analysis["weaknesses"].append(f"{test_type} ååé‡è¾ƒä½ ({throughput:.1f} æ“ä½œ/ç§’)")
            
            # åˆ†æç“¶é¢ˆ
            metrics = result.get("metrics", {})
            if "average_scenario_duration" in metrics and \
               float(metrics["average_scenario_duration"]) > 10.0:
                analysis["bottlenecks"].append(f"{test_type} åœºæ™¯æ‰§è¡Œæ—¶é—´è¾ƒé•¿")
        
        # åˆ†ææœºä¼š
        if "migration_learning" in test_results and \
           test_results["migration_learning"]["status"] == "completed":
            accuracy: float = float(test_results["migration_learning"]["metrics"].get("average_accuracy", 0))
            if accuracy < 0.85:
                analysis["opportunities"].append("è¿ç§»å­¦ä¹ ç²¾åº¦æœ‰æå‡ç©ºé—´")
        
        if "edge_computing" in test_results and \
           test_results["edge_computing"]["status"] == "completed":
            latency_reduction: float = float(test_results["edge_computing"]["metrics"].get("latency_reduction_percentage", 0))
            if latency_reduction < 20.0:
                analysis["opportunities"].append("è¾¹ç¼˜è®¡ç®—å»¶è¿Ÿé™ä½æ•ˆæœå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        return analysis
    
    def _generate_recommendations(self, report: dict[str, Any]) -> list[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        recommendations: list[str] = []
        analysis: dict[str, list[str]] = report["performance_analysis"]
        overall_score: float = float(report["benchmark_info"]["overall_score"])
        
        # åŸºäºæ•´ä½“å¾—åˆ†
        if overall_score < 0.5:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½éœ€è¦é‡å¤§ä¼˜åŒ–ï¼Œå»ºè®®é‡æ–°è¯„ä¼°æ¶æ„è®¾è®¡")
        elif overall_score < 0.7:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½æœ‰è¾ƒå¤§æå‡ç©ºé—´ï¼Œå»ºè®®è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–")
        
        # åŸºäºå¼±ç‚¹åˆ†æ
        for weakness in analysis["weaknesses"]:
            if "æˆåŠŸç‡è¾ƒä½" in weakness:
                recommendations.append("ä¼˜åŒ–é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œæé«˜æ“ä½œæˆåŠŸç‡")
            elif "ååé‡è¾ƒä½" in weakness:
                recommendations.append("ä¼˜åŒ–å¹¶å‘å¤„ç†èƒ½åŠ›ï¼Œæé«˜ç³»ç»Ÿååé‡")
        
        # åŸºäºç“¶é¢ˆåˆ†æ
        for bottleneck in analysis["bottlenecks"]:
            if "åœºæ™¯æ‰§è¡Œæ—¶é—´è¾ƒé•¿" in bottleneck:
                recommendations.append("ä¼˜åŒ–ç®—æ³•æ•ˆç‡ï¼Œå‡å°‘åœºæ™¯æ‰§è¡Œæ—¶é—´")
        
        # åŸºäºæœºä¼šåˆ†æ
        for opportunity in analysis["opportunities"]:
            if "è¿ç§»å­¦ä¹ ç²¾åº¦" in opportunity:
                recommendations.append("ä¼˜åŒ–è¿ç§»å­¦ä¹ å‚æ•°ï¼Œæé«˜æ¨¡å‹ç²¾åº¦")
            elif "è¾¹ç¼˜è®¡ç®—å»¶è¿Ÿ" in opportunity:
                recommendations.append("ä¼˜åŒ–è¾¹ç¼˜èµ„æºåˆ†é…ï¼Œè¿›ä¸€æ­¥é™ä½å»¶è¿Ÿ")
        
        # ç¯å¢ƒç‰¹å®šå»ºè®®
        environment: str = str(report["benchmark_info"]["environment"])
        if environment == "production":
            recommendations.append("ç”Ÿäº§ç¯å¢ƒå»ºè®®å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–åŠŸèƒ½")
        elif environment == "edge":
            recommendations.append("è¾¹ç¼˜ç¯å¢ƒå»ºè®®ä¼˜åŒ–èµ„æºä½¿ç”¨æ•ˆç‡")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿ç§»å­¦ä¹ å’Œè¾¹ç¼˜è®¡ç®—é›†æˆæ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--environment', '-e', 
                       choices=['development', 'testing', 'staging', 'production', 'edge'],
                       default='production',
                       help='æµ‹è¯•ç¯å¢ƒ')
    parser.add_argument('--strategy', '-s',
                       choices=['performance', 'accuracy', 'resource_efficiency', 'latency', 'cost'],
                       default='performance',
                       help='ä¼˜åŒ–ç­–ç•¥')
    parser.add_argument('--output', '-o',
                       default='performance_benchmark_report.json',
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¯ è¿ç§»å­¦ä¹ å’Œè¾¹ç¼˜è®¡ç®—é›†æˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print(f"ç¯å¢ƒ: {args.environment}")
    print(f"ä¼˜åŒ–ç­–ç•¥: {args.strategy}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print()
    
    try:
        # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
        runner = PerformanceBenchmarkRunner(args.environment, args.strategy)
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_results = await runner.run_comprehensive_benchmark()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        detailed_report = runner.generate_detailed_report(benchmark_results)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print("=" * 40)
        print(f"ç»¼åˆå¾—åˆ†: {detailed_report['benchmark_info']['overall_score']:.3f}")
        print(f"æ€§èƒ½ç­‰çº§: {detailed_report['benchmark_info']['performance_level']}")
        print(f"æµ‹è¯•è€—æ—¶: {detailed_report['benchmark_info']['total_duration']:.1f} ç§’")
        
        print("\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
        for test_type, result in detailed_report["test_results"].items():
            status_icon = "âœ…" if result["status"] == "completed" else "âŒ"
            if result["status"] == "completed":
                print(f"  {status_icon} {test_type}: "
                      f"æˆåŠŸç‡ {result['success_rate']:.1%}, "
                      f"ååé‡ {result['throughput']:.1f} æ“ä½œ/ç§’")
            else:
                print(f"  {status_icon} {test_type}: å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        print("\nğŸ’¡ ä¸»è¦å»ºè®®:")
        for i, recommendation in enumerate(detailed_report["recommendations"][:3], 1):
            print(f"  {i}. {recommendation}")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        
        # è¯¦ç»†æ¨¡å¼è¾“å‡º
        if args.verbose:
            print("\n" + "=" * 60)
            print("è¯¦ç»†æ€§èƒ½åˆ†æ:")
            print("=" * 60)
            
            analysis = detailed_report["performance_analysis"]
            for category, items in analysis.items():
                if items:
                    print(f"\n{category.upper()}:")
                    for item in items:
                        print(f"  â€¢ {item}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)